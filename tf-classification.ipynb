{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e3b12d-ecf7-4995-9a24-50d9c0177b7e",
   "metadata": {},
   "source": [
    "## 1. 建立並保存 ONNX 模型檔案\n",
    "以下是一個使用 TensorFlow 建立鳶尾花（Iris）分類模型並將其導出為 ONNX 格式的範例。該模型使用簡單的全連接層來進行分類，並轉換為 ONNX 格式，方便在 TVM 或其他 ONNX 支持的推理引擎上運行。\n",
    "\n",
    "### 1.1 安裝必要的套件\n",
    "如果尚未安裝 tensorflow 和 tf2onnx，可以使用以下命令安裝：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d277e745-0d29-4070-9a60-a5d4b1976ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: tf2onnx in /opt/conda/lib/python3.9/site-packages (1.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /opt/conda/lib/python3.9/site-packages (from tf2onnx) (1.16.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from tf2onnx) (2.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (2023.7.22)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba6d0b-54b1-4aac-bc67-2981917f4e77",
   "metadata": {},
   "source": [
    "### 1.2 建立並訓練 TensorFlow 模型\n",
    "以下程式碼將建立一個簡單的神經網絡來分類鳶尾花數據集，並將其導出為 ONNX 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc99edeb-2521-497d-a7b3-568f78658725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 20:44:13.967297: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 20:44:14.005158: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 20:44:14.005176: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 20:44:14.005201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 20:44:14.012298: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 20:44:14.012894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 20:44:14.882381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-12 20:44:16.201172: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型準確率: 0.97\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 載入鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target\n",
    "\n",
    "# 分割資料集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(4,)),  # 4 個特徵\n",
    "    tf.keras.layers.Dense(10, activation='relu'),  # 隱藏層\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # 輸出層，3 個分類\n",
    "])\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "\n",
    "# 評估模型\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"模型準確率: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8f4e0-2f4e-45c0-8084-2d4a103ce2bf",
   "metadata": {},
   "source": [
    "### 1.3 將模型轉換為 ONNX 格式\n",
    "使用 tf2onnx 將訓練好的 TensorFlow 模型轉換為 ONNX 格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4db139-3d34-41df-a060-0f60f28a628f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 模型已保存至 tf_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 20:44:18.133091: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-11-12 20:44:18.133236: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-11-12 20:44:18.155487: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-11-12 20:44:18.155599: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# 將 Keras 模型轉換為 ONNX 格式\n",
    "spec = (tf.TensorSpec((None, 4), tf.float32, name=\"float_input\"),)  # 定義輸入規範\n",
    "output_path = \"tf_model.onnx\"  # 輸出 ONNX 模型的路徑\n",
    "\n",
    "# 轉換模型\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())\n",
    "\n",
    "print(f\"ONNX 模型已保存至 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1187866b-1998-429a-8ed3-d7c7a09dac36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[5.2434858e-04, 8.7534554e-02, 9.1194111e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# 加載 ONNX 模型\n",
    "session = ort.InferenceSession('tf_model.onnx')\n",
    "\n",
    "# 準備輸入資料\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "\n",
    "# 進行推理\n",
    "pred_onnx = session.run(None, {input_name: input_data})\n",
    "\n",
    "# 輸出預測結果\n",
    "print(pred_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8c39a-af55-4f03-b102-7fbe7f2165cb",
   "metadata": {},
   "source": [
    "### 1.4 TVM輸出共享庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9cb2ab-21f6-455c-8b2d-7b5ee23241f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# original_platform = sys.platform\n",
    "# sys.platform = \"linux\"\n",
    "\n",
    "# # 恢復原始平台\n",
    "# sys.platform = original_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0cc77cc-193e-42a5-b3a7-09a3b9df2374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:04:06] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[21:04:06] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[21:04:06] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc, utils\n",
    "from tvm.contrib import graph_executor\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱可在 ONNX 模型中確認\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構，這裡假設為通用的 CPU\n",
    "target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=x86_64-linux-gnu\")\n",
    "# target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "\n",
    "# 編譯輸出共享庫 \n",
    "lib.export_library(\"output.so\", cc=\"gcc\")\n",
    "# lib.export_library(\"output.so\", cc=\"aarch64-linux-gnu-gcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e6cbb5-ce9d-439b-97f4-6202da098856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf output.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb27f667-bd63-4422-be5b-264e89eb6882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:594:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG\" redefined\n",
      "  594 | #define LOG(level) LOG_##level\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:263:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  263 | #define LOG(severity) LOG_##severity.stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:597:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_FATAL\" redefined\n",
      "  597 | #define LOG_FATAL ::tvm::runtime::detail::LogFatal(__FILE__, __LINE__).stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:257:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  257 | #define LOG_FATAL dmlc::LogMessageFatal(__FILE__, __LINE__)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:598:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_INFO\" redefined\n",
      "  598 | #define LOG_INFO ::tvm::runtime::detail::LogMessage(__FILE__, __LINE__, TVM_LOG_LEVEL_INFO).stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:253:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  253 | #define LOG_INFO dmlc::LogMessage(__FILE__, __LINE__)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:599:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_ERROR\" redefined\n",
      "  599 | #define LOG_ERROR \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:255:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  255 | #define LOG_ERROR LOG_INFO\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:601:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_WARNING\" redefined\n",
      "  601 | #define LOG_WARNING \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:256:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  256 | #define LOG_WARNING LOG_INFO\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:609:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK\" redefined\n",
      "  609 | #define CHECK(x)                                                \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:211:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  211 | #define CHECK(x)                                           \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:614:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_LT\" redefined\n",
      "  614 | #define CHECK_LT(x, y) TVM_CHECK_BINARY_OP(_LT, <, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:215:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  215 | #define CHECK_LT(x, y) CHECK_BINARY_OP(_LT, <, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:615:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_GT\" redefined\n",
      "  615 | #define CHECK_GT(x, y) TVM_CHECK_BINARY_OP(_GT, >, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:216:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  216 | #define CHECK_GT(x, y) CHECK_BINARY_OP(_GT, >, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:616:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_LE\" redefined\n",
      "  616 | #define CHECK_LE(x, y) TVM_CHECK_BINARY_OP(_LE, <=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:217:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  217 | #define CHECK_LE(x, y) CHECK_BINARY_OP(_LE, <=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:617:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_GE\" redefined\n",
      "  617 | #define CHECK_GE(x, y) TVM_CHECK_BINARY_OP(_GE, >=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:218:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  218 | #define CHECK_GE(x, y) CHECK_BINARY_OP(_GE, >=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:618:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_EQ\" redefined\n",
      "  618 | #define CHECK_EQ(x, y) TVM_CHECK_BINARY_OP(_EQ, ==, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:219:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  219 | #define CHECK_EQ(x, y) CHECK_BINARY_OP(_EQ, ==, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:619:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_NE\" redefined\n",
      "  619 | #define CHECK_NE(x, y) TVM_CHECK_BINARY_OP(_NE, !=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:220:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  220 | #define CHECK_NE(x, y) CHECK_BINARY_OP(_NE, !=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:620:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_NOTNULL\" redefined\n",
      "  620 | #define CHECK_NOTNULL(x)                                                          \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:221:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  221 | #define CHECK_NOTNULL(x) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:625:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_IF\" redefined\n",
      "  625 | #define LOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:265:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  265 | #define LOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:651:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_DFATAL\" redefined\n",
      "  651 | #define LOG_DFATAL LOG_ERROR\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:270:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  270 | #define LOG_DFATAL LOG_FATAL\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:652:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DFATAL\" redefined\n",
      "  652 | #define DFATAL ERROR\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:271:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  271 | #define DFATAL FATAL\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:653:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DLOG\" redefined\n",
      "  653 | #define DLOG(severity) true ? (void)0 : ::tvm::runtime::detail::LogMessageVoidify() & LOG(severity)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:272:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  272 | #define DLOG(severity) LOG_IF(severity, ::dmlc::DebugLoggingEnabled())\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:654:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DLOG_IF\" redefined\n",
      "  654 | #define DLOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:273:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  273 | #define DLOG_IF(severity, condition) LOG_IF(severity, ::dmlc::DebugLoggingEnabled() && (condition))\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:670:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"VLOG\" redefined\n",
      "  670 | #define VLOG(level)                                                               \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:261:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  261 | #define VLOG(x) LOG_INFO.stream()\n",
      "      | \n"
     ]
    }
   ],
   "source": [
    "!g++ -std=c++17 -o main tf_inference.cpp \\\n",
    "    -I../tvm/include \\\n",
    "    -I../tvm/3rdparty/dlpack/include \\\n",
    "    -I../tvm/3rdparty/dmlc-core/include \\\n",
    "    ../tvm/build/libtvm_runtime.so \\\n",
    "    -ldl -pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad374f-e8d5-4025-bd0d-5d3c0df0b848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!g++ -std=c++17 -o main tf_inference.cpp \\\n",
    "    -I../tvm/include \\\n",
    "    -I../tvm/3rdparty/dlpack/include \\\n",
    "    -I../tvm/3rdparty/dmlc-core/include \\\n",
    "    -ltvm_runtime -ldl -pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbc2bb1-4a0d-4def-b0e3-ba36cf60e4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Probabilities: [0.000524349, 0.0875346, 0.911941]\n"
     ]
    }
   ],
   "source": [
    "!./main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2f905b-a258-4e37-962a-3dd4115d14b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld: warning: cannot find entry symbol _start; not setting start address\n",
      "ld: ./output.so: undefined reference to `expf'\n"
     ]
    }
   ],
   "source": [
    "# 檢查\n",
    "!ld ./output.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32822d91-5376-4b90-9ba2-d7675f5debda",
   "metadata": {},
   "source": [
    "#### 使用 TVM runtime 加載二進制文件並設置輸入數據，即可執行推論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034635a0-c0b8-440c-b278-d52cbd5c961d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tvm.contrib import graph_executor\n",
    "\n",
    "\n",
    "# 在目標設備上加載二進制文件\n",
    "loaded_lib = tvm.runtime.load_module(\"output.so\")\n",
    "module = graph_executor.GraphModule(loaded_lib[\"default\"](tvm.cpu()))\n",
    "\n",
    "# 準備輸入資料\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "# # 設定輸入數據並執行推論\n",
    "module.set_input(\"float_input\", tvm.nd.array(input_data))\n",
    "module.run()\n",
    "output = module.get_output(0).asnumpy()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9decf-316d-4d48-9fe8-af0a84dbf292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ngrok tcp 0.0.0.0:9090\n",
    "# !python -m tvm.exec.rpc_server --host 0.0.0.0 --port=9090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2949bbf-8c1f-4a15-9a94-7aded6ab01a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import rpc, relay\n",
    "from tvm.contrib import utils, graph_executor\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱可在 ONNX 模型中確認\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標設備的 IP 地址與端口號\n",
    "# remote = rpc.connect(\"目標設備的IP地址\", 9090)\n",
    "remote = rpc.LocalSession()\n",
    "\n",
    "# 在開發機器上編譯模型（例如為 aarch64 設備）\n",
    "target = \"llvm\"\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# 將編譯好的模型傳輸至目標設備\n",
    "temp = utils.tempdir()\n",
    "lib_fname = temp.relpath(\"deploy_lib.tar\")\n",
    "lib.export_library(lib_fname)\n",
    "remote.upload(lib_fname)\n",
    "\n",
    "# 在目標設備上加載模型\n",
    "remote_lib = remote.load_module(\"deploy_lib.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcf226-6f06-455e-97ed-b1a68085b9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f81599-8e2c-42e6-a4db-62d6b732b532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remote_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ff8dc-9f4d-40be-8102-15e488666cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa6535-98ff-4b15-b033-264de41dfd11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 創建 graph executor，使用目標設備的 CPU\n",
    "module = graph_executor.GraphModule(remote_lib[\"default\"](remote.cpu()))\n",
    "\n",
    "# 設置輸入數據\n",
    "# 準備輸入資料\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "# input_data = tvm.nd.array(input_array, device=remote.cpu())  # 輸入的數據\n",
    "module.set_input(\"float_input\", input_data)\n",
    "\n",
    "# 執行推論\n",
    "module.run()\n",
    "\n",
    "# 取得輸出\n",
    "output = module.get_output(0).asnumpy()\n",
    "print(\"推論結果：\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91abfe6-6e13-495e-8deb-30ef3a2bb42a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5e4de-ac3c-4ef6-a2a7-9e1b63b94850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34d4684-c611-447a-8c0f-ed1d137e1f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推論結果： [[0.00405862 0.29633874 0.69960266]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import rpc, relay\n",
    "from tvm.contrib import utils, graph_executor\n",
    "\n",
    "remote = rpc.connect(\"0.tcp.jp.ngrok.io\", 11685)\n",
    "# 在目標設備上加載模型\n",
    "remote_lib = remote.load_module(\"/tmp/tmp7ppa104d/deploy_lib.tar\")\n",
    "\n",
    "# 創建 graph executor，使用目標設備的 CPU\n",
    "module = graph_executor.GraphModule(remote_lib[\"default\"](remote.cpu()))\n",
    "\n",
    "# 設置輸入數據\n",
    "# 準備輸入資料\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "# input_data = tvm.nd.array(input_array, device=remote.cpu())  # 輸入的數據\n",
    "module.set_input(\"float_input\", input_data)\n",
    "\n",
    "# 執行推論\n",
    "module.run()\n",
    "\n",
    "# 取得輸出\n",
    "output = module.get_output(0).asnumpy()\n",
    "print(\"推論結果：\", output)\n",
    "\n",
    "# 釋放資源\n",
    "del module\n",
    "del remote_lib\n",
    "del remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42448fdf-4d70-4447-b0bc-814ccdc19abd",
   "metadata": {},
   "source": [
    "### 1.4 TVM 進行編譯產生 C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b04525-8651-4e29-9294-8e4cdac03c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export TVM_HOME=/home/jovyan/project/ONNX-MLIR/tvm\n",
    "!export PYTHONPATH=$TVM_HOME/python:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b34bd9-7fb7-4727-ae50-de0e0d427ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!TVM_LIBRARY_PATH=/home/jovyan/project/ONNX-MLIR/tvm/build python3 run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d517ed-9ccd-4646-800c-22f341fea275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc\n",
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# Convert ONNX model to Relay format\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape={\"float_input\": (1, 4)})\n",
    "\n",
    "# Compile the model with TVM\n",
    "target = \"c\"\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# Export the compiled library\n",
    "c_source_code = lib.get_lib().get_source()\n",
    "\n",
    "# 將程式碼寫入到一個 .c 檔案\n",
    "with open('output.c', 'w') as file:\n",
    "    file.write(c_source_code)\n",
    "\n",
    "print(\"C source code 已經儲存到 output.c\")\n",
    "\n",
    "# lib.export_library(\"model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94163bf3-af67-4ef5-bf64-834eeb69b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc -o inference output.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064469d-2230-4407-bf15-d05243137229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!g++ output.c -o output -I../tvm/3rdparty/dlpack/include -ltvm_runtime -ldl -lpthread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef650d-4171-4c74-aaf0-54e16d46bd3f",
   "metadata": {},
   "source": [
    "## microTVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db22d8c-54d3-4500-9144-4250bcfcbaea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import utils\n",
    "from tvm.relay.backend import Runtime\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱依據您的 ONNX 模型\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構並啟用 C runtime\n",
    "target = tvm.target.Target(\"llvm -system-lib -mtriple=x86_64-linux-gnu\")\n",
    "runtime = Runtime(\"c\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    # 編譯模型，指定 runtime=runtime 並啟用 system-lib\n",
    "    lib = relay.build(mod, target=target, params=params, runtime=runtime)\n",
    "\n",
    "# 將 C 代碼導出為 model.c\n",
    "lib.export_library(\"model.c\", cc=\"gcc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4c143-965f-4b02-a7e0-964bce3db382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36445887-588c-4332-b609-8c621fb10841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -xvf ./module.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a543d42-711a-414c-89ce-cba1cd6a7b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
