{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e3b12d-ecf7-4995-9a24-50d9c0177b7e",
   "metadata": {},
   "source": [
    "## 1. 建立並保存 ONNX 模型檔案\n",
    "以下是一個使用 TensorFlow 建立鳶尾花（Iris）分類模型並將其導出為 ONNX 格式的範例。該模型使用簡單的全連接層來進行分類，並轉換為 ONNX 格式，方便在 TVM 或其他 ONNX 支持的推理引擎上運行。\n",
    "\n",
    "### 1.1 安裝必要的套件\n",
    "如果尚未安裝 tensorflow 和 tf2onnx，可以使用以下命令安裝：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d277e745-0d29-4070-9a60-a5d4b1976ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: tf2onnx in /opt/conda/lib/python3.9/site-packages (1.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /opt/conda/lib/python3.9/site-packages (from tf2onnx) (1.16.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from tf2onnx) (2.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->tf2onnx) (2023.7.22)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba6d0b-54b1-4aac-bc67-2981917f4e77",
   "metadata": {},
   "source": [
    "### 1.2 建立並訓練 TensorFlow 模型\n",
    "以下程式碼將建立一個簡單的神經網絡來分類鳶尾花數據集，並將其導出為 ONNX 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc99edeb-2521-497d-a7b3-568f78658725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 20:44:13.967297: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 20:44:14.005158: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 20:44:14.005176: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 20:44:14.005201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 20:44:14.012298: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 20:44:14.012894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 20:44:14.882381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-12 20:44:16.201172: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型準確率: 0.97\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 載入鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target\n",
    "\n",
    "# 分割資料集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(4,)),  # 4 個特徵\n",
    "    tf.keras.layers.Dense(10, activation='relu'),  # 隱藏層\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # 輸出層，3 個分類\n",
    "])\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "\n",
    "# 評估模型\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"模型準確率: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8f4e0-2f4e-45c0-8084-2d4a103ce2bf",
   "metadata": {},
   "source": [
    "### 1.3 將模型轉換為 ONNX 格式\n",
    "使用 tf2onnx 將訓練好的 TensorFlow 模型轉換為 ONNX 格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4db139-3d34-41df-a060-0f60f28a628f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# 將 Keras 模型轉換為 ONNX 格式\n",
    "spec = (tf.TensorSpec((None, 4), tf.float32, name=\"float_input\"),)  # 定義輸入規範\n",
    "output_path = \"tf_model.onnx\"  # 輸出 ONNX 模型的路徑\n",
    "\n",
    "# 轉換模型\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())\n",
    "\n",
    "print(f\"ONNX 模型已保存至 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1187866b-1998-429a-8ed3-d7c7a09dac36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[5.2434858e-04, 8.7534554e-02, 9.1194111e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# 加載 ONNX 模型\n",
    "session = ort.InferenceSession('tf_model.onnx')\n",
    "\n",
    "# 準備輸入資料\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "\n",
    "# 進行推理\n",
    "pred_onnx = session.run(None, {input_name: input_data})\n",
    "\n",
    "# 輸出預測結果\n",
    "print(pred_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8c39a-af55-4f03-b102-7fbe7f2165cb",
   "metadata": {},
   "source": [
    "### 1.4 TVM輸出共享庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa9cb2ab-21f6-455c-8b2d-7b5ee23241f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# original_platform = sys.platform\n",
    "# sys.platform = \"linux\"\n",
    "\n",
    "# # 恢復原始平台\n",
    "# sys.platform = original_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0cc77cc-193e-42a5-b3a7-09a3b9df2374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:51:20] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[21:51:20] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[21:51:20] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc, utils\n",
    "from tvm.contrib import graph_executor\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱可在 ONNX 模型中確認\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構，這裡假設為通用的 CPU\n",
    "target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=x86_64-linux-gnu\")\n",
    "# target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "\n",
    "# 編譯輸出共享庫 \n",
    "lib.export_library(\"output.so\", cc=\"gcc\")\n",
    "# lib.export_library(\"output.so\", cc=\"aarch64-linux-gnu-gcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e6cbb5-ce9d-439b-97f4-6202da098856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64K -rwxr-xr-x 1 jovyan users 63K Nov 13 21:51 output.so\n"
     ]
    }
   ],
   "source": [
    "!ls -lhls -lh output.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb27f667-bd63-4422-be5b-264e89eb6882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:594:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG\" redefined\n",
      "  594 | #define LOG(level) LOG_##level\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:263:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  263 | #define LOG(severity) LOG_##severity.stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:597:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_FATAL\" redefined\n",
      "  597 | #define LOG_FATAL ::tvm::runtime::detail::LogFatal(__FILE__, __LINE__).stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:257:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  257 | #define LOG_FATAL dmlc::LogMessageFatal(__FILE__, __LINE__)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:598:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_INFO\" redefined\n",
      "  598 | #define LOG_INFO ::tvm::runtime::detail::LogMessage(__FILE__, __LINE__, TVM_LOG_LEVEL_INFO).stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:253:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  253 | #define LOG_INFO dmlc::LogMessage(__FILE__, __LINE__)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:599:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_ERROR\" redefined\n",
      "  599 | #define LOG_ERROR \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:255:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  255 | #define LOG_ERROR LOG_INFO\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:601:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_WARNING\" redefined\n",
      "  601 | #define LOG_WARNING \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:256:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  256 | #define LOG_WARNING LOG_INFO\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:609:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK\" redefined\n",
      "  609 | #define CHECK(x)                                                \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:211:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  211 | #define CHECK(x)                                           \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:614:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_LT\" redefined\n",
      "  614 | #define CHECK_LT(x, y) TVM_CHECK_BINARY_OP(_LT, <, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:215:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  215 | #define CHECK_LT(x, y) CHECK_BINARY_OP(_LT, <, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:615:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_GT\" redefined\n",
      "  615 | #define CHECK_GT(x, y) TVM_CHECK_BINARY_OP(_GT, >, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:216:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  216 | #define CHECK_GT(x, y) CHECK_BINARY_OP(_GT, >, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:616:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_LE\" redefined\n",
      "  616 | #define CHECK_LE(x, y) TVM_CHECK_BINARY_OP(_LE, <=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:217:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  217 | #define CHECK_LE(x, y) CHECK_BINARY_OP(_LE, <=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:617:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_GE\" redefined\n",
      "  617 | #define CHECK_GE(x, y) TVM_CHECK_BINARY_OP(_GE, >=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:218:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  218 | #define CHECK_GE(x, y) CHECK_BINARY_OP(_GE, >=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:618:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_EQ\" redefined\n",
      "  618 | #define CHECK_EQ(x, y) TVM_CHECK_BINARY_OP(_EQ, ==, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:219:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  219 | #define CHECK_EQ(x, y) CHECK_BINARY_OP(_EQ, ==, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:619:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_NE\" redefined\n",
      "  619 | #define CHECK_NE(x, y) TVM_CHECK_BINARY_OP(_NE, !=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:220:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  220 | #define CHECK_NE(x, y) CHECK_BINARY_OP(_NE, !=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:620:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_NOTNULL\" redefined\n",
      "  620 | #define CHECK_NOTNULL(x)                                                          \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:221:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  221 | #define CHECK_NOTNULL(x) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:625:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_IF\" redefined\n",
      "  625 | #define LOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:265:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  265 | #define LOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:651:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_DFATAL\" redefined\n",
      "  651 | #define LOG_DFATAL LOG_ERROR\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:270:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  270 | #define LOG_DFATAL LOG_FATAL\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:652:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DFATAL\" redefined\n",
      "  652 | #define DFATAL ERROR\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:271:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  271 | #define DFATAL FATAL\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:653:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DLOG\" redefined\n",
      "  653 | #define DLOG(severity) true ? (void)0 : ::tvm::runtime::detail::LogMessageVoidify() & LOG(severity)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:272:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  272 | #define DLOG(severity) LOG_IF(severity, ::dmlc::DebugLoggingEnabled())\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:654:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DLOG_IF\" redefined\n",
      "  654 | #define DLOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:273:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  273 | #define DLOG_IF(severity, condition) LOG_IF(severity, ::dmlc::DebugLoggingEnabled() && (condition))\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/include/tvm/runtime/logging.h:670:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"VLOG\" redefined\n",
      "  670 | #define VLOG(level)                                                               \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ktf_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:261:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  261 | #define VLOG(x) LOG_INFO.stream()\n",
      "      | \n"
     ]
    }
   ],
   "source": [
    "!g++ -std=c++17 -o main tf_inference.cpp \\\n",
    "    -I../tvm/include \\\n",
    "    -I../tvm/3rdparty/dlpack/include \\\n",
    "    -I../tvm/3rdparty/dmlc-core/include \\\n",
    "    ../tvm/build/libtvm_runtime.so \\\n",
    "    -ldl -pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad374f-e8d5-4025-bd0d-5d3c0df0b848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!g++ -std=c++17 -o main tf_inference.cpp \\\n",
    "    -I../tvm/include \\\n",
    "    -I../tvm/3rdparty/dlpack/include \\\n",
    "    -I../tvm/3rdparty/dmlc-core/include \\\n",
    "    -ltvm_runtime -ldl -pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbc2bb1-4a0d-4def-b0e3-ba36cf60e4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Probabilities: [0.000524349, 0.0875346, 0.911941]\n"
     ]
    }
   ],
   "source": [
    "!./main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2f905b-a258-4e37-962a-3dd4115d14b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld: warning: cannot find entry symbol _start; not setting start address\n",
      "ld: ./output.so: undefined reference to `expf'\n"
     ]
    }
   ],
   "source": [
    "# 檢查\n",
    "!ld ./output.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32822d91-5376-4b90-9ba2-d7675f5debda",
   "metadata": {},
   "source": [
    "#### 使用 TVM runtime 加載二進制文件並設置輸入數據，即可執行推論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034635a0-c0b8-440c-b278-d52cbd5c961d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tvm.contrib import graph_executor\n",
    "\n",
    "\n",
    "# 在目標設備上加載二進制文件\n",
    "loaded_lib = tvm.runtime.load_module(\"output.so\")\n",
    "module = graph_executor.GraphModule(loaded_lib[\"default\"](tvm.cpu()))\n",
    "\n",
    "# 準備輸入資料\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "# # 設定輸入數據並執行推論\n",
    "module.set_input(\"float_input\", tvm.nd.array(input_data))\n",
    "module.run()\n",
    "output = module.get_output(0).asnumpy()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921a8d3-3cfa-487c-ba4f-a330769440e7",
   "metadata": {},
   "source": [
    "## TVM優化\n",
    "在使用 TVM 編譯模型時，出現了以下警告訊息：\n",
    "\n",
    "```\n",
    "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n",
    "\n",
    "```\n",
    "\n",
    "這個警告的含義是：\n",
    "- 模型中有一個或多個算子（operators）尚未經過調優（tuning），因此 TVM 使用了預設的調度（schedule）來生成程式碼。\n",
    "- 這些預設的調度可能不是針對您的硬體或模型特性進行優化的，因此可能無法達到最佳性能。\n",
    "- 建議對模型進行調優，以獲得更好的性能。\n",
    "\n",
    "為了解決這個警告並提高模型的性能，您需要使用 TVM 的自動調優工具（AutoTVM 或 AutoScheduler）對模型進行算子級別的調優。\n",
    "\n",
    "### 方法一：使用 AutoTVM 進行調優\n",
    "\n",
    "AutoTVM 是 TVM 提供的自動調優框架，可以自動搜尋最佳的算子實現，以提高模型的運行性能。\n",
    "\n",
    "**步驟**：\n",
    "1. 定義調優任務\n",
    "2. 配置調優選項\n",
    "3. 運行調優任務\n",
    "4. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3a568-a4ce-4b6c-858a-cb673e727511",
   "metadata": {},
   "source": [
    "#### 1. 定義調優任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa5259b-02ea-4e36-959f-0ef1290cd9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay, autotvm\n",
    "from tvm.contrib import graph_executor\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱可在 ONNX 模型中確認\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構，這裡假設為通用的 CPU\n",
    "target = \"llvm\"\n",
    "\n",
    "# 定義調優任務\n",
    "tasks = autotvm.task.extract_from_program(mod[\"main\"], target=target, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88144101-2bff-4db2-9e7f-66bca492de61",
   "metadata": {},
   "source": [
    "#### 2. 配置調優選項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2bf905-978e-4a23-a1ba-91024c7206be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定義日誌文件，用於保存調優記錄\n",
    "log_file = \"tuning.log\"\n",
    "\n",
    "# 定義測量選項\n",
    "measure_option = autotvm.measure_option(\n",
    "    builder=autotvm.LocalBuilder(),\n",
    "    runner=autotvm.LocalRunner(number=10, repeat=1, timeout=10, min_repeat_ms=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d73c4-4c70-4342-8ebb-f9bf3c3bb9d8",
   "metadata": {},
   "source": [
    "#### 3. 運行調優任務\n",
    "注意 n_trial 是調優的總次數，調優次數越多，找到更優解的可能性越大，但耗時也會增加。您可以根據需要調整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c6846-6946-4b62-ae8b-ce0d41552de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tvm.autotvm.tuner import XGBTuner\n",
    "\n",
    "# 對每個任務進行調優\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"===== Tuning task {i+1}/{len(tasks)}: {task.name} =====\")\n",
    "    tuner = XGBTuner(task)\n",
    "    tuner.tune(\n",
    "        n_trial=100,  # 調優次數，可以根據需要調整\n",
    "        measure_option=measure_option,\n",
    "        callbacks=[\n",
    "            autotvm.callback.progress_bar(100),\n",
    "            autotvm.callback.log_to_file(log_file)\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9229ca8-7bf3-4c85-ab07-2e2bb408ac70",
   "metadata": {},
   "source": [
    "#### 4. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c3a3b4-fb99-4d12-bc77-be9ca6b1771f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 套用調優結果\n",
    "with autotvm.apply_history_best(log_file):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# 編譯輸出共享庫\n",
    "lib.export_library(\"output.so\") #優化結果 64K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fdb6dd5-86c1-42ac-94a3-ef8102a07bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64K -rwxr-xr-x 1 jovyan users 62K Nov 13 21:17 \u001b[0m\u001b[01;32moutput.so\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls -lh output.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45458434-10da-4d31-aa93-55585f10bdfa",
   "metadata": {},
   "source": [
    "### 方法二：使用 AutoScheduler 進行調優\n",
    "AutoScheduler 是 TVM 新一代的自動調優框架，能夠更高效地探索優化空間，適用於複雜模型。\n",
    "\n",
    "**步驟**：\n",
    "1. 定義調優任務\n",
    "2. 運行調優\n",
    "3. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dee1f-3132-42ea-9949-cab6fa157176",
   "metadata": {},
   "source": [
    "#### 1. 定義調優任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9553d1e-6f0a-46de-90ff-b1d8d849e201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tvm import auto_scheduler\n",
    "\n",
    "# 設置目標架構\n",
    "target = tvm.target.Target(\"llvm\")\n",
    "\n",
    "# 提取調優任務\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef281467-d099-4aed-938e-4d4e34e72061",
   "metadata": {},
   "source": [
    "#### 2. 運行調優"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1bdaf-24ed-400f-8b55-a33923c9050f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_file = \"auto_scheduler_tuning.json\"\n",
    "\n",
    "def tune_tasks(tasks, task_weights, log_file):\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=100,  # 調優總次數，可以調整\n",
    "        runner=auto_scheduler.LocalRunner(repeat=1, min_repeat_ms=300, timeout=10),\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "    tuner.tune(tune_option)\n",
    "\n",
    "# 開始調優\n",
    "tune_tasks(tasks, task_weights, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b6d4f-f4a9-42a5-a76c-68cb8533ff19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aed88-324b-4cc6-8492-338cf229266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用調優日誌\n",
    "with auto_scheduler.ApplyHistoryBest(log_file):\n",
    "    with tvm.transform.PassContext(opt_level=3,\n",
    "                                   config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# 編譯輸出共享庫\n",
    "lib.export_library(\"output.so\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42448fdf-4d70-4447-b0bc-814ccdc19abd",
   "metadata": {},
   "source": [
    "### 1.4 TVM 進行編譯產生 C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b04525-8651-4e29-9294-8e4cdac03c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export TVM_HOME=/home/jovyan/project/ONNX-MLIR/tvm\n",
    "!export PYTHONPATH=$TVM_HOME/python:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b34bd9-7fb7-4727-ae50-de0e0d427ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!TVM_LIBRARY_PATH=/home/jovyan/project/ONNX-MLIR/tvm/build python3 run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d517ed-9ccd-4646-800c-22f341fea275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc\n",
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# Convert ONNX model to Relay format\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape={\"float_input\": (1, 4)})\n",
    "\n",
    "# Compile the model with TVM\n",
    "target = \"c\"\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# Export the compiled library\n",
    "c_source_code = lib.get_lib().get_source()\n",
    "\n",
    "# 將程式碼寫入到一個 .c 檔案\n",
    "with open('output.c', 'w') as file:\n",
    "    file.write(c_source_code)\n",
    "\n",
    "print(\"C source code 已經儲存到 output.c\")\n",
    "\n",
    "# lib.export_library(\"model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94163bf3-af67-4ef5-bf64-834eeb69b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc -o inference output.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064469d-2230-4407-bf15-d05243137229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!g++ output.c -o output -I../tvm/3rdparty/dlpack/include -ltvm_runtime -ldl -lpthread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef650d-4171-4c74-aaf0-54e16d46bd3f",
   "metadata": {},
   "source": [
    "## microTVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db22d8c-54d3-4500-9144-4250bcfcbaea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import utils\n",
    "from tvm.relay.backend import Runtime\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱依據您的 ONNX 模型\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構並啟用 C runtime\n",
    "target = tvm.target.Target(\"llvm -system-lib -mtriple=x86_64-linux-gnu\")\n",
    "runtime = Runtime(\"c\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    # 編譯模型，指定 runtime=runtime 並啟用 system-lib\n",
    "    lib = relay.build(mod, target=target, params=params, runtime=runtime)\n",
    "\n",
    "# 將 C 代碼導出為 model.c\n",
    "lib.export_library(\"model.c\", cc=\"gcc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4c143-965f-4b02-a7e0-964bce3db382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36445887-588c-4332-b609-8c621fb10841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -xvf ./module.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a543d42-711a-414c-89ce-cba1cd6a7b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
