{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562f5a16-835d-4517-bedc-2aa15b5bda49",
   "metadata": {},
   "source": [
    "## 1. 建立並保存 ONNX 模型檔案\n",
    "以下是一個使用 scikit-learn 建立鳶尾花（Iris）邏輯迴歸分類器並將其導出為 ONNX 格式的範例。\n",
    "\n",
    "### 1.1 建立並訓練邏輯迴歸模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5282405-e6bf-4bbb-aea9-5adfa7351132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 載入鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 分割數據集為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立模型Pipeline：標準化 + 邏輯迴歸\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8ea04-fd6d-4339-8cd8-7904a920259e",
   "metadata": {},
   "source": [
    "### 1.2 將模型轉換為 ONNX 格式\n",
    "使用 skl2onnx 將訓練好的 scikit-learn 模型轉換為 ONNX 格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da65a942-c764-4c6b-bbed-813d00ec51b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 模型已儲存至 ../tf-example/logistic_model.onnx\n"
     ]
    }
   ],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# 定義輸入類型\n",
    "initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]\n",
    "\n",
    "# 轉換為 ONNX 模型\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=9)\n",
    "\n",
    "# 指定儲存路徑\n",
    "onnx_file_path = \"logistic_model.onnx\"\n",
    "\n",
    "# 將 ONNX 模型儲存為檔案\n",
    "with open(onnx_file_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"ONNX 模型已儲存至 {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae98f28-7c4b-4b2e-906d-2a4731843cb1",
   "metadata": {},
   "source": [
    "#### 1.2.1 使用ONNX Runtime進行推論測試\n",
    "我們可以先透過 ONNX Runtime 輸入一筆測試資料檢查推論結果。可以跟稍後 ONNX-MLIR 推論結果進行驗證比較看有沒有數值一至。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa57338-f3ec-4c89-9225-67b5b5f5e81a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 2.4887262043193914e-05, 1: 0.008561260998249054, 2: 0.9914138317108154}]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# 加載 ONNX 模型\n",
    "session = ort.InferenceSession('logistic_model.onnx')\n",
    "\n",
    "# 準備輸入資料\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_data = np.array([[6.3, 3.3, 6. , 2.5]], dtype=np.float32)\n",
    "\n",
    "# 進行推理\n",
    "pred_onnx = session.run(None, {input_name: input_data})[1]\n",
    "\n",
    "# 輸出預測結果\n",
    "print(pred_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afb752-fe98-435f-a2c0-efe30ed2ee55",
   "metadata": {},
   "source": [
    "### 1.3 使用 Hummingbird 將模型轉換為 ONNX 格式\n",
    "TVM 的設計主要針對深度學習（DL）模型，例如 CNN、RNN 等神經網路，這些模型的特點是以張量（Tensor）為主要數據結構。 Logistic Regression 模型轉換為 ONNX 時，輸出的預測結果默認是 `Sequence<Map>` 類型，用於將分類概率與標籤對應起來。然而，這些類型並不是 TVM 所支持的張量類型，因此引發錯誤。\n",
    "\n",
    "> OpNotImplemented: The following operators are not supported for frontend ONNX: ZipMap, Scaler, LinearClassifier, Normalizer\n",
    "\n",
    "一個有效的解決方案是使用 Hummingbird，這是一個專門將傳統機器學習模型轉換為神經網路框架（如 PyTorch）的工具。 Hummingbird 可以將 Logistic Regression、Random Forest 等 scikit-learn 模型轉換為 PyTorch 模型。 轉換後的模型具有純張量輸入輸出結構，非常適合使用 TVM 編譯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b26da0-0c0d-4103-97dc-ebf902a47ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install onnxruntime==1.19.2 onnx==1.16.1 hummingbird-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75efcac-0b6c-4c66-a248-40349081498b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model saved with digest: f96b10de0b3cf9f3886187ec9e352468abd4b7cf\n",
      "Archive:  onnx-tmp.zip\n",
      "  inflating: dist/model_type.txt     \n",
      "  inflating: dist/container.pkl      \n",
      "  inflating: dist/model_configuration.txt  \n",
      "  inflating: dist/deploy_model.onnx  \n"
     ]
    }
   ],
   "source": [
    "from hummingbird.ml import convert\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load('logistic_model.onnx')\n",
    "# 將模型轉換為 Hummingbird 格式\n",
    "hb_model = convert(onnx_model, 'onnx')\n",
    "# 保存轉換後的 ONNX 模型\n",
    "hb_model.save('onnx-tmp')\n",
    "\n",
    "# 解壓縮資料夾\n",
    "!unzip -o onnx-tmp.zip -d dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf359b62-6a92-4d18-a6f5-e9f1bded23c4",
   "metadata": {},
   "source": [
    "### 1.3.1 ONNX Runtime 執行推論\n",
    "使用 ONNX Runtime 執行推論 hummingbird 轉換後的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5121bc-2e91-40f5-ac50-6d7412a0f0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4887262e-05 8.5612610e-03 9.9141383e-01]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# 加載 ONNX 模型\n",
    "session = ort.InferenceSession('./dist/deploy_model.onnx')\n",
    "\n",
    "# 準備輸入資料\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_data = np.array([[6.3, 3.3, 6. , 2.5]], dtype=np.float32)\n",
    "\n",
    "# 進行推理\n",
    "pred_onnx = session.run(None, {input_name: input_data})[1]\n",
    "\n",
    "# 輸出預測結果\n",
    "print(pred_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935d0af-0524-458c-9b77-12106ed592db",
   "metadata": {},
   "source": [
    "## 2. 使用 TVM 轉換模型為共享庫\n",
    "### 2.1 將 ONNX 模型編譯為共享庫\n",
    "使用 TVM relay 將 tf_model.onnx 模型轉換為共享庫（.so 文件）。\n",
    "\n",
    "- 載入模型：從 ONNX 格式載入預訓練的模型。\n",
    "- 模型轉換：將 ONNX 模型轉換為 TVM 的 Relay 模組，方便進行優化和編譯。\n",
    "- 圖優化：應用高級優化策略，提高模型效率。\n",
    "- 中間表示轉換：將優化後的 Relay 模組轉換為 LLVM 的中間表示。\n",
    "- 代碼生成：從 LLVM IR 生成位元碼，編譯為機器代碼。\n",
    "- 連結與生成庫：將機器代碼連結成最終的共享庫，供部署使用。\n",
    "\n",
    "> 成功輸出後即可前往第3撰寫 C++ 程式進行推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40308bf-b78a-4deb-b362-06da95164506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:11:06] /home/jovyan/project/ONNX-MLIR/tvm/src/te/schedule/bound.cc:119: not in feed graph consumer = compute(p0_red_temp, body=[T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=0), T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=1)], axis=[T.iter_var(ax0, T.Range(0, 1), \"DataPar\", \"\")], reduce_axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], tag=comm_reduce_idx, attrs={})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.03 s, sys: 1.48 s, total: 3.51 s\n",
      "Wall time: 834 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc, utils\n",
    "import onnx\n",
    "import sys\n",
    "\n",
    "original_platform = sys.platform\n",
    "sys.platform = \"linux\"\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"./dist/deploy_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱可在 ONNX 模型中確認\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構，這裡假設為通用的 CPU\n",
    "# target = 'llvm'\n",
    "target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=x86_64-linux-gnu\")\n",
    "# target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "# target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=x86_64-apple-darwin\")\n",
    "with tvm.transform.PassContext(opt_level=1):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "\n",
    "# 編譯輸出為 C 代碼\n",
    "lib.export_library(\"output.so\", cc=\"x86_64-linux-gnu-gcc\")\n",
    "# lib.export_library(\"output.so\", cc=\"aarch64-linux-gnu-gcc\")\n",
    "# lib.export_library(\"output.so\", cc=\"clang\")\n",
    "\n",
    "# 恢復原始平台\n",
    "sys.platform = original_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c24ca3-1633-47df-8f7d-dccda0ee71f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108K\toutput.so\n"
     ]
    }
   ],
   "source": [
    "!du -h output.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8b638-df7f-41ef-a761-944348d89444",
   "metadata": {},
   "source": [
    "如果要在 macOS 跨平台編譯。在 macOS 上使用 TVM 的export_library函數時，預設會包含-undefined dynamic_lookup連結器標誌。會導致連結器報錯：\n",
    "> Command line: x86_64-linux-gnu-gcc -shared -fPIC -undefined dynamic_lookup -o output.so \n",
    "\n",
    "遇到的錯誤是由於 macOS 特有的連結器標誌 -undefineddynamic_lookup 被傳遞給了 Linux 交叉編譯器，導致編譯失敗。為了解決這個問題，可以暫時修改sys.platform，讓 TVM 認為正在 Linux 上執行：\n",
    "\n",
    "```py\n",
    "import sys\n",
    "\n",
    "original_platform = sys.platform\n",
    "sys.platform = \"linux\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc23522-abae-4ba0-98df-35ce38a2b04c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 使用 TVM Python API 推論(optional)\n",
    "使用 TVM runtime 載入共享庫並設置輸入數據，即可執行推論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0c3a8e-f335-4019-9bde-4c3ddae2d848",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[[2.4887262e-05 8.5612610e-03 9.9141383e-01]]\n"
     ]
    }
   ],
   "source": [
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "\n",
    "# 在目標設備上載入共享庫\n",
    "loaded_lib = tvm.runtime.load_module(\"output.so\")\n",
    "module = graph_executor.GraphModule(loaded_lib[\"default\"](tvm.cpu()))\n",
    "\n",
    "# 準備輸入資料\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "# # 設定輸入數據並執行推論\n",
    "module.set_input(\"float_input\", tvm.nd.array(input_data))\n",
    "module.run()\n",
    "print(module.get_output(0).asnumpy())\n",
    "print(module.get_output(1).asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d6828-28f8-42f7-befb-42cd6bff87d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 使用TVM推論(optional)\n",
    "若無法順利產so 因此直接使用TVM推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6374cda-4abf-4469-b313-c548e03f36a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:43:23] /home/jovyan/project/ONNX-MLIR/tvm/src/te/schedule/bound.cc:119: not in feed graph consumer = compute(p0_red_temp, body=[T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=0), T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=1)], axis=[T.iter_var(ax0, T.Range(0, 1), \"DataPar\", \"\")], reduce_axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], tag=comm_reduce_idx, attrs={})\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"./dist/deploy_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 TVM 的 Relay 模型格式\n",
    "input_name = 'float_input'  # 確認模型的輸入名稱，可從 ONNX 模型中查看\n",
    "shape_dict = {input_name: (1, 4)}  # 定義輸入形狀 (1, 4) 代表輸入的形狀\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設定編譯目標為 CPU 上的 LLVM (可以根據需求設為 \"cuda\" 或 \"opencl\" 等其他架構)\n",
    "target = \"llvm\"\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    # 編譯 Relay 模型，將其轉換為可以運行的格式\n",
    "    lib = relay.build(mod, target=target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c4f7d9a-3420-41a1-a1fc-9d3f2b17a398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4887262e-05 8.5612610e-03 9.9141383e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 使用 TVM 的 Graph Executor 模組來載入編譯後的模型\n",
    "module = graph_executor.GraphModule(lib[\"default\"](tvm.cpu(0)))\n",
    "\n",
    "# 定義輸入資料並設定模型輸入，這裡使用一筆測試資料\n",
    "input_data = np.array([[6.3, 3.3, 6. , 2.5]], dtype=np.float32)\n",
    "module.set_input(input_name, input_data)\n",
    "\n",
    "# 執行模型推理\n",
    "module.run()\n",
    "\n",
    "# 定義輸出形狀，這裡假設模型的輸出形狀為 (1, 1)\n",
    "output_shape = (1, 1)  # 可根據實際模型調整\n",
    "tvm_output = module.get_output(1).asnumpy()  # 獲取輸出並轉換為 NumPy 陣列\n",
    "\n",
    "# 輸出推理結果\n",
    "print(tvm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d9c8db6-7aa9-4f0a-93bd-66668b7fcc60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "# 打印輸出形狀\n",
    "tvm_output = module.get_output(0).asnumpy()\n",
    "print(\"Output shape:\", tvm_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e6fb2-835e-4191-bcd8-95c4a54f4246",
   "metadata": {},
   "source": [
    "## 3. 撰寫 C++ 程式進行推論\n",
    "\n",
    "### 3.1 撰寫 C++ 程式\n",
    "\n",
    "> 請參考 sklearn-example/sk_inference.cpp\n",
    "\n",
    "### 3.2 編譯程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de69962-ac91-4f92-aca2-44aa246ba968",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:594:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG\" redefined\n",
      "  594 | #define LOG(level) LOG_##level\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:263:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  263 | #define LOG(severity) LOG_##severity.stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:597:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_FATAL\" redefined\n",
      "  597 | #define LOG_FATAL ::tvm::runtime::detail::LogFatal(__FILE__, __LINE__).stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:257:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  257 | #define LOG_FATAL dmlc::LogMessageFatal(__FILE__, __LINE__)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:598:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_INFO\" redefined\n",
      "  598 | #define LOG_INFO ::tvm::runtime::detail::LogMessage(__FILE__, __LINE__, TVM_LOG_LEVEL_INFO).stream()\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:253:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  253 | #define LOG_INFO dmlc::LogMessage(__FILE__, __LINE__)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:599:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_ERROR\" redefined\n",
      "  599 | #define LOG_ERROR \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:255:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  255 | #define LOG_ERROR LOG_INFO\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:601:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_WARNING\" redefined\n",
      "  601 | #define LOG_WARNING \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:256:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  256 | #define LOG_WARNING LOG_INFO\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:609:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK\" redefined\n",
      "  609 | #define CHECK(x)                                                \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:211:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  211 | #define CHECK(x)                                           \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:614:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_LT\" redefined\n",
      "  614 | #define CHECK_LT(x, y) TVM_CHECK_BINARY_OP(_LT, <, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:215:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  215 | #define CHECK_LT(x, y) CHECK_BINARY_OP(_LT, <, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:615:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_GT\" redefined\n",
      "  615 | #define CHECK_GT(x, y) TVM_CHECK_BINARY_OP(_GT, >, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:216:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  216 | #define CHECK_GT(x, y) CHECK_BINARY_OP(_GT, >, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:616:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_LE\" redefined\n",
      "  616 | #define CHECK_LE(x, y) TVM_CHECK_BINARY_OP(_LE, <=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:217:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  217 | #define CHECK_LE(x, y) CHECK_BINARY_OP(_LE, <=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:617:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_GE\" redefined\n",
      "  617 | #define CHECK_GE(x, y) TVM_CHECK_BINARY_OP(_GE, >=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:218:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  218 | #define CHECK_GE(x, y) CHECK_BINARY_OP(_GE, >=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:618:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_EQ\" redefined\n",
      "  618 | #define CHECK_EQ(x, y) TVM_CHECK_BINARY_OP(_EQ, ==, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:219:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  219 | #define CHECK_EQ(x, y) CHECK_BINARY_OP(_EQ, ==, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:619:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_NE\" redefined\n",
      "  619 | #define CHECK_NE(x, y) TVM_CHECK_BINARY_OP(_NE, !=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:220:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  220 | #define CHECK_NE(x, y) CHECK_BINARY_OP(_NE, !=, x, y)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:620:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"CHECK_NOTNULL\" redefined\n",
      "  620 | #define CHECK_NOTNULL(x)                                                          \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:221:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  221 | #define CHECK_NOTNULL(x) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:625:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_IF\" redefined\n",
      "  625 | #define LOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:265:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  265 | #define LOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:651:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"LOG_DFATAL\" redefined\n",
      "  651 | #define LOG_DFATAL LOG_ERROR\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:270:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  270 | #define LOG_DFATAL LOG_FATAL\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:652:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DFATAL\" redefined\n",
      "  652 | #define DFATAL ERROR\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:271:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  271 | #define DFATAL FATAL\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:653:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DLOG\" redefined\n",
      "  653 | #define DLOG(severity) true ? (void)0 : ::tvm::runtime::detail::LogMessageVoidify() & LOG(severity)\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:272:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  272 | #define DLOG(severity) LOG_IF(severity, ::dmlc::DebugLoggingEnabled())\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:654:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"DLOG_IF\" redefined\n",
      "  654 | #define DLOG_IF(severity, condition) \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:273:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  273 | #define DLOG_IF(severity, condition) LOG_IF(severity, ::dmlc::DebugLoggingEnabled() && (condition))\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/base.h:28\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/container/string.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:31\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../../tvm/include/tvm/runtime/logging.h:670:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"VLOG\" redefined\n",
      "  670 | #define VLOG(level)                                                               \\\n",
      "      | \n",
      "In file included from \u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/io.h:15\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K../../tvm/include/tvm/runtime/module.h:29\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Ksk_inference.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K../..//tvm/3rdparty/dmlc-core/include/dmlc/./logging.h:261:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
      "  261 | #define VLOG(x) LOG_INFO.stream()\n",
      "      | \n",
      "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kunrecognized command-line option ‘\u001b[01m\u001b[K-Wno-macro-redefined\u001b[m\u001b[K’ may have been intended to silence earlier diagnostics\n"
     ]
    }
   ],
   "source": [
    "!g++ -std=c++17 -o main sk_inference.cpp \\\n",
    "    -I../../tvm/include \\\n",
    "    -I../..//tvm/3rdparty/dlpack/include \\\n",
    "    -I../..//tvm/3rdparty/dmlc-core/include \\\n",
    "    -ltvm_runtime -ldl -pthread \\\n",
    "    -Wno-macro-redefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f755cdc2-8aed-4939-a5ef-8066d21a05fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Label: 2\n",
      "Prediction Probabilities: [2.48873e-05, 0.00856126, 0.991414]\n"
     ]
    }
   ],
   "source": [
    "!./main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36910af5-eeeb-4e8c-843f-6b00242cd33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:36:37] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[14:36:37] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[14:36:37] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "/home/jovyan/project/ONNX-MLIR/tvm/python/tvm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tvm; print(tvm.__file__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8651b7d-7b38-485c-b178-c866abd5461d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. TVM其他功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f8b96-d070-4450-8af9-d0f1b20112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache-tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa8260c6-a54b-4c6d-b186-0fb3a0c17008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: tvmc: command not found\n"
     ]
    }
   ],
   "source": [
    "!tvmc compile ./dist/deploy_model.onnx --target=\"llvm\" --output model.tar "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535416ca-e9c0-43cf-a97f-0ef408df6258",
   "metadata": {},
   "source": [
    "### 4.1 Micro TVM \n",
    "產生三個檔案，但不知道怎模用。\n",
    "- lib1.c\n",
    "- devc.c\n",
    "- lib0.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ac90ae1-224c-4c81-94ec-6f0f9798b8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:18:21] /home/jovyan/project/ONNX-MLIR/tvm/src/te/schedule/bound.cc:119: not in feed graph consumer = compute(p0_red_temp, body=[T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=0), T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=1)], axis=[T.iter_var(ax0, T.Range(0, 1), \"DataPar\", \"\")], reduce_axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], tag=comm_reduce_idx, attrs={})\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"./modified_model.onnx\")\n",
    "\n",
    "# 定義輸入資訊\n",
    "input_name = 'float_input'\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模組\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設定編譯目標為產生 C 原始碼\n",
    "target = tvm.target.Target(\"c\")\n",
    "\n",
    "# 使用 AOT 執行器\n",
    "executor = tvm.relay.build_module.Executor(\"aot\")\n",
    "\n",
    "# 編譯 Relay 模組\n",
    "with tvm.transform.PassContext(opt_level=3, config={\"tir.disable_vectorize\": True}):\n",
    "    lib = relay.build(mod, target=target, executor=executor, params=params)\n",
    "\n",
    "lib.export_library(\"c_model.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e0b9a3e-eeff-4c92-b4de-5c07e0a5250b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lib1.c\n",
      "devc.c\n",
      "lib0.c\n"
     ]
    }
   ],
   "source": [
    "!tar xvf c_model.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963aaa7-a130-439a-a8e1-7404d70ba4e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 TVM 進行編譯產生\n",
    "產生了兩個檔案，但不知怎利用。\n",
    "\n",
    "- devc.c\n",
    "- lib0.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f43fff27-45e7-4182-814c-1e65cef22be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共享庫已儲存\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:08:57] /home/jovyan/project/ONNX-MLIR/tvm/src/te/schedule/bound.cc:119: not in feed graph consumer = compute(p0_red_temp, body=[T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=0), T.reduce(T.comm_reducer(lambda argmax_lhs_0, argmax_lhs_1, argmax_rhs_0, argmax_rhs_1: (T.Select(argmax_lhs_1 > argmax_rhs_1 or argmax_lhs_1 == argmax_rhs_1 and argmax_lhs_0 < argmax_rhs_0, argmax_lhs_0, argmax_rhs_0), T.Select(argmax_lhs_1 > argmax_rhs_1, argmax_lhs_1, argmax_rhs_1)), [-1, T.float32(-340282346638528859811704183484516925440.0)]), source=[k1, p0[ax0, k1]], init=[], axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], condition=T.bool(True), value_index=1)], axis=[T.iter_var(ax0, T.Range(0, 1), \"DataPar\", \"\")], reduce_axis=[T.iter_var(k1, T.Range(0, 3), \"CommReduce\", \"\")], tag=comm_reduce_idx, attrs={})\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc, utils, graph_executor\n",
    "import onnx\n",
    "import numpy as np\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"./modified_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 確認模型的輸入名稱\n",
    "shape_dict = {input_name: (1, 4)}  # 定義輸入形狀 (1, 4)\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設定編譯目標為 \"c\" 以生成 C 程式碼\n",
    "target = \"c\"\n",
    "with tvm.transform.PassContext(opt_level=3, config={\"tir.disable_vectorize\": True}):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# # 儲存編譯的共享庫 (dylib 為 macOS 共享庫)\n",
    "# lib.export_library(\"logistic_regression_iris.so\", cc.create_shared, cc=\"g++\")\n",
    "lib.export_library(\"c_model.tar\")\n",
    "print(f\"共享庫已儲存\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb1ea0-1b5e-4299-9cdc-a8ed51333bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb1cc831-008b-43d7-870c-b6cebf26dd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devc.c\n",
      "lib0.c\n"
     ]
    }
   ],
   "source": [
    "!tar xvf c_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e5c69-3105-4418-88db-f6401c22f3db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
