{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e3b12d-ecf7-4995-9a24-50d9c0177b7e",
   "metadata": {},
   "source": [
    "## 1. 建立並保存 ONNX 模型檔案\n",
    "以下是一個使用 TensorFlow 建立鳶尾花（Iris）分類模型並將其導出為 ONNX 格式的範例。該模型使用簡單的全連接層來進行分類，並轉換為 ONNX 格式，方便在 TVM 或其他 ONNX 支持的推理引擎上運行。\n",
    "\n",
    "### 1.1 安裝必要的套件\n",
    "如果尚未安裝 tensorflow 和 tf2onnx，可以使用以下命令安裝：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277e745-0d29-4070-9a60-a5d4b1976ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba6d0b-54b1-4aac-bc67-2981917f4e77",
   "metadata": {},
   "source": [
    "### 1.2 建立並訓練 TensorFlow 模型\n",
    "以下程式碼將建立一個簡單的神經網絡來分類鳶尾花數據集，並將其導出為 ONNX 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99edeb-2521-497d-a7b3-568f78658725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 載入鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target\n",
    "\n",
    "# 分割資料集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(4,)),  # 4 個特徵\n",
    "    tf.keras.layers.Dense(10, activation='relu'),  # 隱藏層\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # 輸出層，3 個分類\n",
    "])\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "\n",
    "# 評估模型\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"模型準確率: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8f4e0-2f4e-45c0-8084-2d4a103ce2bf",
   "metadata": {},
   "source": [
    "### 1.3 將模型轉換為 ONNX 格式\n",
    "使用 tf2onnx 將訓練好的 TensorFlow 模型轉換為 ONNX 格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4db139-3d34-41df-a060-0f60f28a628f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# 將 Keras 模型轉換為 ONNX 格式\n",
    "spec = (tf.TensorSpec((None, 4), tf.float32, name=\"float_input\"),)  # 定義輸入規範\n",
    "output_path = \"tf_model.onnx\"  # 輸出 ONNX 模型的路徑\n",
    "\n",
    "# 轉換模型\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())\n",
    "\n",
    "print(f\"ONNX 模型已保存至 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1187866b-1998-429a-8ed3-d7c7a09dac36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[5.2434858e-04, 8.7534554e-02, 9.1194111e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# 加載 ONNX 模型\n",
    "session = ort.InferenceSession('tf_model.onnx')\n",
    "\n",
    "# 準備輸入資料\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "\n",
    "# 進行推理\n",
    "pred_onnx = session.run(None, {input_name: input_data})\n",
    "\n",
    "# 輸出預測結果\n",
    "print(pred_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8c39a-af55-4f03-b102-7fbe7f2165cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. 使用 TVM 轉換模型為共享庫\n",
    "### 2.1 將 ONNX 模型編譯為共享庫\n",
    "使用 TVM relay 將 tf_model.onnx 模型轉換為共享庫（.so 文件）。\n",
    "\n",
    "> 成功輸出後即可前往第3撰寫 C++ 程式進行推論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132ad41-f25a-4191-a981-732006bc1ce7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 補充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ce7ae-2b2e-4f01-9020-30dedb2424de",
   "metadata": {
    "tags": []
   },
   "source": [
    "在 TVM 中，`opt_level` 參數用於控制編譯過程中的優化程度。不同的優化 Pass（即優化步驟）被分配到不同的優化等級。當您設定特定的 `opt_level` 時，TVM 會執行該等級及以下所有對應的優化 Pass。\n",
    "\n",
    "**`opt_level=0`**\n",
    "執行基本的優化：\n",
    "- `SimplifyInference`：簡化推理過程。\n",
    "\n",
    "**`opt_level=1`**\n",
    "包含等級 0 的優化，並新增：\n",
    "- `OpFusion`：運算符融合，將多個運算符合併以提高效率。\n",
    "\n",
    "**`opt_level=2`**\n",
    "包含等級 0 和 1 的優化，並新增：\n",
    "- `FoldConstant`：常量折疊，預先計算常量表達式以減少運算量。\n",
    "\n",
    "**`opt_level=3`**\n",
    "包含等級 0、1 和 2 的優化，並新增：\n",
    "- `FoldScaleAxis`：折疊縮放軸。\n",
    "- `AlterOpLayout`：改變運算符的佈局以適應特定硬體。\n",
    "- `CanonicalizeOps`：規範化運算符。\n",
    "- `CanonicalizeCast`：規範化類型轉換。\n",
    "- `EliminateCommonSubexpr`：消除公共子表達式以減少重複計算。\n",
    "\n",
    "**`opt_level=4`**\n",
    "包含等級 0 至 3 的優化，並新增：\n",
    "- `CombineParallelConv2D`：合併平行的 2D 卷積運算。\n",
    "- `CombineParallelDense`：合併平行的全連接層運算。\n",
    "- `CombineParallelBatchMatmul`：合併平行的批次矩陣乘法運算。\n",
    "- `FastMath`：啟用快速數學運算，可能會犧牲精度以換取速度。\n",
    "\n",
    "- 參考:[tvm.relay.build_config](https://tvm.apache.org/docs/reference/api/python/relay/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0cc77cc-193e-42a5-b3a7-09a3b9df2374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 2.08 s, total: 4.28 s\n",
      "Wall time: 489 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc, utils\n",
    "from tvm.contrib import graph_executor\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱可在 ONNX 模型中確認\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構，這裡假設為通用的 CPU\n",
    "target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=x86_64-linux-gnu\")\n",
    "# target = tvm.target.Target(\"llvm\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=0):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "\n",
    "# 編譯輸出共享庫 \n",
    "lib.export_library(\"output.so\", cc=\"gcc\")\n",
    "# lib.export_library(\"output.so\", cc=\"aarch64-linux-gnu-gcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c873fc0-fef8-4a61-a78c-c990c0ea445f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92K\toutput.so\n"
     ]
    }
   ],
   "source": [
    "!du -h output.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32822d91-5376-4b90-9ba2-d7675f5debda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 使用 TVM Python API 推論(optional)\n",
    "使用 TVM runtime 載入共享庫並設置輸入數據，即可執行推論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034635a0-c0b8-440c-b278-d52cbd5c961d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.243493e-04, 8.753461e-02, 9.119410e-01]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "\n",
    "# 在目標設備上載入共享庫\n",
    "loaded_lib = tvm.runtime.load_module(\"output.so\")\n",
    "module = graph_executor.GraphModule(loaded_lib[\"default\"](tvm.cpu()))\n",
    "\n",
    "# 準備輸入資料\n",
    "input_data = np.array([[6.3, 3.3, 6.0, 2.5]], dtype=np.float32)\n",
    "# # 設定輸入數據並執行推論\n",
    "module.set_input(\"float_input\", tvm.nd.array(input_data))\n",
    "module.run()\n",
    "output = module.get_output(0).asnumpy()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47b952c-c917-4e20-9c5d-9bb3c4f4b151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(GraphExecutorFactory, 558a6671faf8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1dcda-e108-4fb3-b8fb-bb482035b4f4",
   "metadata": {},
   "source": [
    "## 3. 撰寫 C++ 程式進行推論\n",
    "\n",
    "### 3.1 撰寫 C++ 程式\n",
    "\n",
    "> 請參考 tf-example/tf_inference.cpp\n",
    "\n",
    "### 3.2 編譯程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27f667-bd63-4422-be5b-264e89eb6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -std=c++17 -o main tf_inference.cpp \\\n",
    "    -I../../tvm/include \\\n",
    "    -I../../tvm/3rdparty/dlpack/include \\\n",
    "    -I../../tvm/3rdparty/dmlc-core/include \\\n",
    "    ../../tvm/build/libtvm_runtime.so \\\n",
    "    -ldl -pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad374f-e8d5-4025-bd0d-5d3c0df0b848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!g++ -std=c++17 -o main tf_inference.cpp \\\n",
    "    -I../../tvm/include \\\n",
    "    -I../../tvm/3rdparty/dlpack/include \\\n",
    "    -I../../tvm/3rdparty/dmlc-core/include \\\n",
    "    -ltvm_runtime -ldl -pthread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc411d3e-222d-4499-8f7c-c1479dfb7144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=$(pwd):$LD_LIBRARY_PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbc2bb1-4a0d-4def-b0e3-ba36cf60e4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Probabilities: [0.000524349, 0.0875346, 0.911941]\n"
     ]
    }
   ],
   "source": [
    "!./main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2f905b-a258-4e37-962a-3dd4115d14b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tstatically linked\n"
     ]
    }
   ],
   "source": [
    "# 檢查相依庫\n",
    "!ldd ./output.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3ea0c9-0c04-4217-98ab-c320e90d493b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlibtvm_runtime.so (libc6,x86-64) => /usr/local/lib/libtvm_runtime.so\n"
     ]
    }
   ],
   "source": [
    "# 檢查共享庫位置\n",
    "!ldconfig -p | grep libtvm_runtime.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880b9fa-388e-43b4-b3fe-6218d57aa41d",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [TVM 編譯 ONNX 模型](https://tvm.hyper.ai/docs/how_to/compile/compile_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921a8d3-3cfa-487c-ba4f-a330769440e7",
   "metadata": {},
   "source": [
    "## TVM優化\n",
    "在使用 TVM 編譯模型時，出現了以下警告訊息：\n",
    "\n",
    "```\n",
    "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n",
    "\n",
    "```\n",
    "\n",
    "這個警告的含義是：\n",
    "- 模型中有一個或多個算子（operators）尚未經過調優（tuning），因此 TVM 使用了預設的調度（schedule）來生成程式碼。\n",
    "- 這些預設的調度可能不是針對您的硬體或模型特性進行優化的，因此可能無法達到最佳性能。\n",
    "- 建議對模型進行調優，以獲得更好的性能。\n",
    "\n",
    "為了解決這個警告並提高模型的性能，您需要使用 TVM 的自動調優工具（AutoTVM 或 AutoScheduler）對模型進行算子級別的調優。\n",
    "\n",
    "### 方法一：使用 AutoTVM 進行調優\n",
    "\n",
    "AutoTVM 是 TVM 提供的自動調優框架，可以自動搜尋最佳的算子實現，以提高模型的運行性能。\n",
    "\n",
    "**步驟**：\n",
    "1. 定義調優任務\n",
    "2. 配置調優選項\n",
    "3. 運行調優任務\n",
    "4. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3a568-a4ce-4b6c-858a-cb673e727511",
   "metadata": {},
   "source": [
    "#### 1. 定義調優任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa5259b-02ea-4e36-959f-0ef1290cd9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay, autotvm\n",
    "from tvm.contrib import graph_executor\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'  # 輸入名稱可在 ONNX 模型中確認\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構，這裡假設為通用的 CPU\n",
    "target = \"llvm\"\n",
    "\n",
    "# 定義調優任務\n",
    "tasks = autotvm.task.extract_from_program(mod[\"main\"], target=target, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88144101-2bff-4db2-9e7f-66bca492de61",
   "metadata": {},
   "source": [
    "#### 2. 配置調優選項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2bf905-978e-4a23-a1ba-91024c7206be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定義日誌文件，用於保存調優記錄\n",
    "log_file = \"tuning.log\"\n",
    "\n",
    "# 定義測量選項\n",
    "measure_option = autotvm.measure_option(\n",
    "    builder=autotvm.LocalBuilder(),\n",
    "    runner=autotvm.LocalRunner(number=10, repeat=1, timeout=10, min_repeat_ms=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d73c4-4c70-4342-8ebb-f9bf3c3bb9d8",
   "metadata": {},
   "source": [
    "#### 3. 運行調優任務\n",
    "注意 n_trial 是調優的總次數，調優次數越多，找到更優解的可能性越大，但耗時也會增加。您可以根據需要調整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c6846-6946-4b62-ae8b-ce0d41552de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tvm.autotvm.tuner import XGBTuner\n",
    "\n",
    "# 對每個任務進行調優\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"===== Tuning task {i+1}/{len(tasks)}: {task.name} =====\")\n",
    "    tuner = XGBTuner(task)\n",
    "    tuner.tune(\n",
    "        n_trial=100,  # 調優次數，可以根據需要調整\n",
    "        measure_option=measure_option,\n",
    "        callbacks=[\n",
    "            autotvm.callback.progress_bar(100),\n",
    "            autotvm.callback.log_to_file(log_file)\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9229ca8-7bf3-4c85-ab07-2e2bb408ac70",
   "metadata": {},
   "source": [
    "#### 4. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c3a3b4-fb99-4d12-bc77-be9ca6b1771f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 套用調優結果\n",
    "with autotvm.apply_history_best(log_file):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# 編譯輸出共享庫\n",
    "lib.export_library(\"output.so\") #優化結果 64K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fdb6dd5-86c1-42ac-94a3-ef8102a07bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64K\toutput.so\n"
     ]
    }
   ],
   "source": [
    "!du -h output.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45458434-10da-4d31-aa93-55585f10bdfa",
   "metadata": {},
   "source": [
    "### 方法二：使用 AutoScheduler 進行調優\n",
    "AutoScheduler 是 TVM 新一代的自動調優框架，能夠更高效地探索優化空間，適用於複雜模型。\n",
    "\n",
    "**步驟**：\n",
    "1. 定義調優任務\n",
    "2. 運行調優\n",
    "3. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dee1f-3132-42ea-9949-cab6fa157176",
   "metadata": {},
   "source": [
    "#### 1. 定義調優任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9553d1e-6f0a-46de-90ff-b1d8d849e201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tvm import auto_scheduler\n",
    "\n",
    "# 設置目標架構\n",
    "target = tvm.target.Target(\"llvm\")\n",
    "\n",
    "# 提取調優任務\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef281467-d099-4aed-938e-4d4e34e72061",
   "metadata": {},
   "source": [
    "#### 2. 運行調優"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1bdaf-24ed-400f-8b55-a33923c9050f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_file = \"auto_scheduler_tuning.json\"\n",
    "\n",
    "def tune_tasks(tasks, task_weights, log_file):\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=100,  # 調優總次數，可以調整\n",
    "        runner=auto_scheduler.LocalRunner(repeat=1, min_repeat_ms=300, timeout=10),\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "    tuner.tune(tune_option)\n",
    "\n",
    "# 開始調優\n",
    "tune_tasks(tasks, task_weights, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b6d4f-f4a9-42a5-a76c-68cb8533ff19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. 使用調優結果編譯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aed88-324b-4cc6-8492-338cf229266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用調優日誌\n",
    "with auto_scheduler.ApplyHistoryBest(log_file):\n",
    "    with tvm.transform.PassContext(opt_level=3,\n",
    "                                   config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# 編譯輸出共享庫\n",
    "lib.export_library(\"output.so\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42448fdf-4d70-4447-b0bc-814ccdc19abd",
   "metadata": {},
   "source": [
    "### 1.4 TVM 進行編譯產生 C(Bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b04525-8651-4e29-9294-8e4cdac03c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export TVM_HOME=/home/jovyan/project/ONNX-MLIR/tvm\n",
    "!export PYTHONPATH=$TVM_HOME/python:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b34bd9-7fb7-4727-ae50-de0e0d427ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!TVM_LIBRARY_PATH=/home/jovyan/project/ONNX-MLIR/tvm/build python3 run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca01e6b3-12dc-4687-9bed-83740cf0845b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:49:59] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[14:49:59] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[14:49:59] /home/jovyan/project/ONNX-MLIR/tvm/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.3 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"./tf_model.onnx\")\n",
    "\n",
    "# 定義輸入資訊\n",
    "input_name = 'float_input'\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模組\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設定編譯目標為產生 C 原始碼\n",
    "target = tvm.target.Target(\"c\")\n",
    "\n",
    "# 使用 AOT 執行器\n",
    "executor = tvm.relay.build_module.Executor(\"aot\")\n",
    "\n",
    "# 編譯 Relay 模組\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, executor=executor, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b58f77-4072-45c2-9ba1-64eb182684d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c8e19b5-6ed8-49d0-bb8d-8100994ca739",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 測試有Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba852395-a13e-43af-ae61-ce41065640ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import cc\n",
    "import onnx\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"tf_model.onnx\")\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模型\n",
    "input_name = 'float_input'\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# 設置目標架構\n",
    "target = tvm.target.Target(\"c\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=1):\n",
    "    # 編譯 Relay 模型\n",
    "    mod = relay.transform.SimplifyInference()(mod)\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# Export the compiled library\n",
    "c_source_code = lib.get_lib().get_source()\n",
    "\n",
    "# 將程式碼寫入到一個 .c 檔案\n",
    "with open('output.c', 'w') as file:\n",
    "    file.write(c_source_code)\n",
    "print(\"C source code 已經儲存到 output.c\")\n",
    "\n",
    "# lib.export_library(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94163bf3-af67-4ef5-bf64-834eeb69b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file included from \u001b[01m\u001b[Koutput.c:3\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/include/tvm/runtime/c_runtime_api.h:79:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kdlpack/dlpack.h: No such file or directory\n",
      "   79 | #include \u001b[01;31m\u001b[K<dlpack/dlpack.h>\u001b[m\u001b[K\n",
      "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "compilation terminated.\n"
     ]
    }
   ],
   "source": [
    "!gcc -o inference output.c -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7064469d-2230-4407-bf15-d05243137229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[Koutput.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint32_t tvmgen_default_fused_nn_contrib_dense_pack_add(void*, int32_t*, int32_t, void*, int32_t*, void*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Koutput.c:56:3:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Kfloat3\u001b[m\u001b[K’ was not declared in this scope; did you mean ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’?\n",
      "   56 |   \u001b[01;31m\u001b[Kfloat3\u001b[m\u001b[K compute_global[1];\n",
      "      |   \u001b[01;31m\u001b[K^~~~~~\u001b[m\u001b[K\n",
      "      |   \u001b[32m\u001b[Kfloat\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:57:3:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Kcompute_global\u001b[m\u001b[K’ was not declared in this scope\n",
      "   57 |   \u001b[01;31m\u001b[Kcompute_global\u001b[m\u001b[K[0] = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      |   \u001b[01;31m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:59:144:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected primary-expression before ‘\u001b[01m\u001b[K)\u001b[m\u001b[K’ token\n",
      "   59 | float*)p0_1)[k_outer], ((float*)p0_1)[k_outer])) * *(float3*\u001b[01;31m\u001b[K)\u001b[m\u001b[K(((float*)p1_1) + (k_outer * 3))));\n",
      "      |                                                             \u001b[01;31m\u001b[K^\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[Koutput.c:61:12:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected primary-expression before ‘\u001b[01m\u001b[K)\u001b[m\u001b[K’ token\n",
      "   61 |   *(float3*\u001b[01;31m\u001b[K)\u001b[m\u001b[K(((float*)T_add_1) + 0) = (compute_global[0] + *(float3*)(((float*)p2_1) + 0));\n",
      "      |            \u001b[01;31m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:61:69:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected primary-expression before ‘\u001b[01m\u001b[K)\u001b[m\u001b[K’ token\n",
      "   61 | t3*)(((float*)T_add_1) + 0) = (compute_global[0] + *(float3*\u001b[01;31m\u001b[K)\u001b[m\u001b[K(((float*)p2_1) + 0));\n",
      "      |                                                             \u001b[01;31m\u001b[K^\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[Koutput.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint32_t tvmgen_default_fused_nn_contrib_dense_pack_add_nn_relu(void*, int32_t*, int32_t, void*, int32_t*, void*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Koutput.c:100:5:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Kfloat5\u001b[m\u001b[K’ was not declared in this scope; did you mean ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’?\n",
      "  100 |     \u001b[01;31m\u001b[Kfloat5\u001b[m\u001b[K compute_global[1];\n",
      "      |     \u001b[01;31m\u001b[K^~~~~~\u001b[m\u001b[K\n",
      "      |     \u001b[32m\u001b[Kfloat\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:101:5:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Kcompute_global\u001b[m\u001b[K’ was not declared in this scope\n",
      "  101 |     \u001b[01;31m\u001b[Kcompute_global\u001b[m\u001b[K[0] = ((float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n",
      "      |     \u001b[01;31m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:103:196:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected primary-expression before ‘\u001b[01m\u001b[K)\u001b[m\u001b[K’ token\n",
      "  103 | float*)p0_1)[k_outer], ((float*)p0_1)[k_outer])) * *(float5*\u001b[01;31m\u001b[K)\u001b[m\u001b[K(((float*)p1_1) + ((ax1_outer_ax0_outer_fused * 20) + (k_outer * 5)))));\n",
      "      |                                                             \u001b[01;31m\u001b[K^\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[Koutput.c:105:5:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Kint32_t5\u001b[m\u001b[K’ was not declared in this scope; did you mean ‘\u001b[01m\u001b[Kint32_t\u001b[m\u001b[K’?\n",
      "  105 |     \u001b[01;31m\u001b[Kint32_t5\u001b[m\u001b[K v_ = int32_t5((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4));\n",
      "      |     \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "      |     \u001b[32m\u001b[Kint32_t\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:106:11:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ before ‘\u001b[01m\u001b[Kv__1\u001b[m\u001b[K’\n",
      "  106 |     float5\u001b[01;31m\u001b[K \u001b[m\u001b[K\u001b[32m\u001b[Kv__1\u001b[m\u001b[K = compute_global[0] + (float5(((float*)p2_1)[v_.s0],((float*)p2_1)[v_.s1],((float*)p2_1)[v_.s2],((float*)p2_1)[v_.s3],((float*)p2_1)[v_.s4]));\n",
      "      |           \u001b[01;31m\u001b[K^\u001b[m\u001b[K\u001b[32m\u001b[K~~~~\u001b[m\u001b[K\n",
      "      |           \u001b[32m\u001b[K;\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:107:11:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ before ‘\u001b[01m\u001b[Kv__2\u001b[m\u001b[K’\n",
      "  107 |     float5\u001b[01;31m\u001b[K \u001b[m\u001b[K\u001b[32m\u001b[Kv__2\u001b[m\u001b[K = (float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f);\n",
      "      |           \u001b[01;31m\u001b[K^\u001b[m\u001b[K\u001b[32m\u001b[K~~~~\u001b[m\u001b[K\n",
      "      |           \u001b[32m\u001b[K;\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:108:14:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected primary-expression before ‘\u001b[01m\u001b[K)\u001b[m\u001b[K’ token\n",
      "  108 |     *(float5*\u001b[01;31m\u001b[K)\u001b[m\u001b[K(((float*)T_relu_1) + cse_var_1) = ((v__1) > (v__2) ? (v__1) : (v__2));\n",
      "      |              \u001b[01;31m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:108:52:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Kv__1\u001b[m\u001b[K’ was not declared in this scope\n",
      "  108 |     *(float5*)(((float*)T_relu_1) + cse_var_1) = ((\u001b[01;31m\u001b[Kv__1\u001b[m\u001b[K) > (v__2) ? (v__1) : (v__2));\n",
      "      |                                                    \u001b[01;31m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Koutput.c:108:61:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Kv__2\u001b[m\u001b[K’ was not declared in this scope\n",
      "  108 |     *(float5*)(((float*)T_relu_1) + cse_var_1) = ((v__1) > (\u001b[01;31m\u001b[Kv__2\u001b[m\u001b[K) ? (v__1) : (v__2));\n",
      "      |                                                             \u001b[01;31m\u001b[K^~~~\u001b[m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "!g++ output.c -o output -I../../tvm/3rdparty/dlpack/include -ltvm_runtime -ldl -lpthread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef650d-4171-4c74-aaf0-54e16d46bd3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## microTVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11c3b07b-877a-400c-a820-99c08a11c015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('microtvm_model.tar')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.micro import export_model_library_format\n",
    "\n",
    "# 載入 ONNX 模型\n",
    "onnx_model = onnx.load(\"./tf_model.onnx\")\n",
    "\n",
    "# 定義輸入資訊\n",
    "input_name = 'float_input'\n",
    "shape_dict = {input_name: (1, 4)}\n",
    "\n",
    "# 將 ONNX 模型轉換為 Relay 模組\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "\n",
    "\n",
    "# 設定目標為嵌入式設備，使用 MicroTVM\n",
    "target = tvm.target.target.micro(\"host\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, executor=executor, params=params)\n",
    "\n",
    "\n",
    "# 導出為模型庫格式，包含所有生成的源代碼\n",
    "export_model_library_format(lib, \"microtvm_model.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b58147f3-2b3e-4b84-9e3b-fab25d7282d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./codegen/\n",
      "./codegen/host/\n",
      "./codegen/host/src/\n",
      "./codegen/host/src/default_lib0.c\n",
      "./codegen/host/src/default_lib1.c\n",
      "./metadata.json\n",
      "./parameters/\n",
      "./parameters/default.params\n",
      "./src/\n",
      "./src/default.relay\n"
     ]
    }
   ],
   "source": [
    "!mkdir build\n",
    "!tar xvf microtvm_model.tar -C build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a543d42-711a-414c-89ce-cba1cd6a7b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
